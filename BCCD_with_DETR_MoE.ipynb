{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OxyzGiaHuy/BCCD/blob/main/BCCD_with_DETR_MoE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DETR vs DETR-MoE Comparison on BCCD Dataset\n",
        "\n",
        "This notebook compares the performance of the standard DETR model with a modified version incorporating Mixture of Experts (MoE) layers in the Transformer's Feed-Forward Networks (FFNs). The comparison is performed on the Blood Cell Count and Detection (BCCD) dataset.\n",
        "\n",
        "**Steps:**\n",
        "1. Setup: Install libraries, clone DETR repo.\n",
        "2. Dataset: Download BCCD, prepare Dataset/DataLoader.\n",
        "3. Models: Define Standard DETR and DETR-MoE for BCCD.\n",
        "4. Training & Evaluation Utilities: Implement training and COCO evaluation loops.\n",
        "5. Experiment 1: Train and evaluate standard DETR.\n",
        "6. Experiment 2: Train and evaluate DETR-MoE.\n",
        "7. Comparison: Summarize results."
      ],
      "metadata": {
        "id": "TExOwCy75RTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Shenggan/BCCD_Dataset.git"
      ],
      "metadata": {
        "id": "goJZdiQr6mYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers datasets evaluate timm albumentations pycocotools"
      ],
      "metadata": {
        "id": "j7VWhnLu6Lpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4TLY2Kw43k2"
      },
      "outputs": [],
      "source": [
        "# %% Setup - Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from transformers import DetrImageProcessor, DetrConfig, DetrForObjectDetection, Trainer, TrainingArguments\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc # Garbage collection\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BCCDDataset(Dataset):\n",
        "    \"\"\"\n",
        "    BCCD Dataset Class.\n",
        "    Reads images and annotations (Pascal VOC format) from the BCCD dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dir, annot_dir, image_set_path, processor, transforms=None, name_to_id=None):\n",
        "        self.image_dir = Path(image_dir)\n",
        "        self.annot_dir = Path(annot_dir)\n",
        "        self.processor = processor\n",
        "        self.transforms = transforms\n",
        "        self.name_to_id = name_to_id if name_to_id else {'RBC': 1, 'WBC': 2, 'Platelets': 3}\n",
        "        self.id_to_name = {v: k for k, v in self.name_to_id.items()}\n",
        "        self.id_to_label = {0: 'background'} | self.id_to_name\n",
        "        self.label_to_id = {k: v for v, k in self.id_to_label.items()}\n",
        "\n",
        "        with open(image_set_path, \"r\") as f:\n",
        "            self.image_ids_str = [line.strip() for line in f.readlines()]\n",
        "\n",
        "        self.annotations = self._load_annotations()\n",
        "\n",
        "        self.image_ids_str = [img_id for img_id in self.image_ids_str if img_id in self.annotations]\n",
        "        print(f\"Loaded {len(self.image_ids_str)} images from {image_set_path}\")\n",
        "        print(f\"Class mapping (excluding background for file loading): {self.name_to_id}\")\n",
        "        print(f\"Full label map (including background for model): {self.id_to_label}\")\n",
        "        print(f\"First 5 image IDs: {self.image_ids_str[:5]}\")\n",
        "\n",
        "\n",
        "    def _load_annotations(self):\n",
        "        annotations = {}\n",
        "        for image_id_str in tqdm(self.image_ids_str, desc=\"Loading Annotations\"):\n",
        "            annot_path = self.annot_dir / f\"{image_id_str}.xml\"\n",
        "            if not annot_path.exists():\n",
        "                #  print(f\"Warning: Annotation file not found {annot_path}\")\n",
        "                 annotations[image_id_str] = {'boxes': torch.empty((0, 4), dtype=torch.float32),\n",
        "                                              'labels': torch.empty((0,), dtype=torch.int64),\n",
        "                                              'size': (0,0)}\n",
        "                 continue\n",
        "\n",
        "            tree = ET.parse(annot_path)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            size_elem = root.find(\"size\")\n",
        "            img_w = int(size_elem.find(\"width\").text)\n",
        "            img_h = int(size_elem.find(\"height\").text)\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            for obj in root.findall(\"object\"):\n",
        "                name = obj.find(\"name\").text\n",
        "                if name not in self.name_to_id:\n",
        "                    #  print(f\"Warning: Unknown class '{name}' in {annot_path}. Skipping.\")\n",
        "                     continue # Skip unknown classes\n",
        "\n",
        "                bndbox = obj.find(\"bndbox\")\n",
        "                xmin = int(bndbox.find(\"xmin\").text)\n",
        "                ymin = int(bndbox.find(\"ymin\").text)\n",
        "                xmax = int(bndbox.find(\"xmax\").text)\n",
        "                ymax = int(bndbox.find(\"ymax\").text)\n",
        "\n",
        "                # Basic validation\n",
        "                if xmin >= xmax or ymin >= ymax or xmin < 0 or ymin < 0 or xmax > img_w or ymax > img_h:\n",
        "                    print(f\"Warning: Invalid box coordinates {name}: {[xmin, ymin, xmax, ymax]} in {annot_path} (Image size {img_w}x{img_h}). Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                if xmax - xmin <= 1 or ymax - ymin <= 1: # Skip tiny boxes\n",
        "                    # print(f\"Warning: Tiny box {name}: {[xmin, ymin, xmax, ymax]} in {annot_path}. Skipping.\") # Too verbose\n",
        "                    continue\n",
        "\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(self.name_to_id[name])\n",
        "\n",
        "            annotations[image_id_str] = {\n",
        "                'boxes': torch.tensor(boxes, dtype=torch.float32) if boxes else torch.empty((0, 4), dtype=torch.float32),\n",
        "                'labels': torch.tensor(labels, dtype=torch.int64) if labels else torch.empty((0,), dtype=torch.int64),\n",
        "                'size': torch.tensor([img_h, img_w], dtype=torch.int64) # Store as H, W tensor\n",
        "            }\n",
        "        return annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids_str)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id_str = self.image_ids_str[idx]\n",
        "        img_path = self.image_dir / f\"{image_id_str}.jpg\"\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        # width, height = image.size\n",
        "        original_size = self.annotations[image_id_str]['size'].clone().detach().to(dtype=torch.int64) # Use size from loaded annotation\n",
        "\n",
        "\n",
        "        # Ensure annotations were loaded correctly\n",
        "        if image_id_str not in self.annotations:\n",
        "             # This shouldn't happen after filtering image_ids_str in __init__\n",
        "             # but good practice to raise error if it does.\n",
        "             raise KeyError(f\"Annotations for {image_id_str} not found in pre-loaded dict.\")\n",
        "\n",
        "        target = self.annotations[image_id_str]\n",
        "\n",
        "        # Convert boxes from [xmin, ymin, xmax, ymax] (Pascal VOC) to required format\n",
        "        # The processor expects {'image_id': int, 'annotations': [{'bbox': [xmin, ymin, width, height], 'category_id': int, 'area': float, 'iscrowd': 0}]}\n",
        "        # Note: The HuggingFace processor handles box conversion internally if given in COCO format!\n",
        "        formatted_annotations = []\n",
        "        for box, label in zip(target['boxes'], target['labels']):\n",
        "            xmin, ymin, xmax, ymax = box.tolist()\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "            # COCO format: [xmin, ymin, width, height]\n",
        "            if width > 0 and height > 0: # Only add valid boxes\n",
        "                 area = width * height\n",
        "                 formatted_annotations.append({\n",
        "                     \"bbox\": [xmin, ymin, width, height], # COCO format\n",
        "                     \"category_id\": label.item(),\n",
        "                     \"area\": area,\n",
        "                     \"iscrowd\": 0,\n",
        "                     \"image_id\": image_id_str # <-- FIX: Use the original image_id_str here\n",
        "                 })\n",
        "\n",
        "        target_dict_for_processor = {\n",
        "            \"image_id\": idx, # <-- FIX: Use the original image_id_str here too (redundant but good)\n",
        "            \"annotations\": formatted_annotations,\n",
        "            \"size\": original_size.tolist() # Processor expects size as list [H, W]\n",
        "        }\n",
        "\n",
        "        # Apply Albumentations transforms if provided (using Pascal VOC format for input)\n",
        "        # Note: Albumentations outputs will also be in Pascal VOC format\n",
        "        img_np = np.array(image)\n",
        "        boxes_pascal = target['boxes'].tolist()\n",
        "        labels_list = target['labels'].tolist()\n",
        "\n",
        "        # Apply Albumentations transforms if provided\n",
        "        if self.transforms:\n",
        "            try:\n",
        "                transform_input = {\n",
        "                    'image': img_np,\n",
        "                    'bboxes': boxes_pascal,\n",
        "                    'class_labels': labels_list\n",
        "                }\n",
        "                transformed = self.transforms(**transform_input)\n",
        "                img_np = transformed['image']\n",
        "                transformed_boxes_pascal = transformed['bboxes']\n",
        "                transformed_labels_list = transformed['class_labels']\n",
        "\n",
        "                # Reformat transformed boxes/labels back into the target dictionary for the processor\n",
        "                formatted_annotations_transformed = []\n",
        "                # new_height, new_width = image.shape[1], image.shape[2] # Get size from tensor C, H, W\n",
        "                for box, label in zip(transformed_boxes_pascal, transformed_labels_list):\n",
        "                    xmin, ymin, xmax, ymax = box\n",
        "                    width = xmax - xmin\n",
        "                    height = ymax - ymin\n",
        "                    if width > 0 and height > 0:\n",
        "                        area = width * height\n",
        "                        formatted_annotations_transformed.append({\n",
        "                            \"bbox\": [xmin, ymin, width, height],\n",
        "                            \"category_id\": label,\n",
        "                            \"area\": area,\n",
        "                            \"iscrowd\": 0,\n",
        "                            \"image_id\": image_id_str\n",
        "                        })\n",
        "                target_dict_for_processor[\"annotations\"] = formatted_annotations_transformed\n",
        "                # The image is already a tensor due to ToTensorV2\n",
        "                # Apply processor normalization etc *after* albumentations augmentation\n",
        "                # But the processor usually expects a PIL image or numpy array.\n",
        "                # Let's convert back for the processor, then it handles final tensor conversion+normalization\n",
        "                # image = Image.fromarray(image.permute(1, 2, 0).numpy()) # Convert back to HWC PIL/numpy format if needed by processor\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying transforms to {image_id_str}: {e}. Using original data.\")\n",
        "                # If transform fails, proceed with original data loaded earlier\n",
        "                img_np = np.array(Image.open(img_path).convert(\"RGB\")) # Ensure img_np is original\n",
        "                target_dict_for_processor[\"annotations\"] = formatted_annotations # Ensure annotations are original\n",
        "\n",
        "        # Use DetrImageProcessor to prepare image and targets for the model\n",
        "        # It handles resizing, normalization, and converts targets to the required format\n",
        "        # {'pixel_values': ..., 'pixel_mask': ..., 'labels': [{'class_labels': ..., 'boxes': ...}] }\n",
        "        encoding = self.processor(images=img_np, annotations=target_dict_for_processor, return_tensors=\"pt\")\n",
        "\n",
        "        # The processor returns data in a batch format, squeeze out the batch dim\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze(0)\n",
        "        pixel_mask = encoding[\"pixel_mask\"].squeeze(0)\n",
        "        labels = encoding[\"labels\"][0] # Access the labels for the first (and only) image in the batch\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"pixel_mask\": pixel_mask,\n",
        "            \"labels\": labels, # This dict contains 'class_labels', 'boxes', 'image_id' (value from target_dict_for_processor)\n",
        "            \"orig_size\": original_size, # Original size tensor [H, W]\n",
        "            \"image_id_str\": image_id_str # <-- Add image_id_str here explicitly for collate/eval\n",
        "            }"
      ],
      "metadata": {
        "id": "rAgCW-kE7RlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Albumentations transforms\n",
        "# DETR models are typically trained with resizing and normalization.\n",
        "# Color jitter is often beneficial.\n",
        "# Note: DETR uses specific normalization constants. The processor handles this.\n",
        "# We focus on geometric augmentations here.\n",
        "\n",
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=10, p=0.3), # Reduced rotate limit\n",
        "        A.RandomBrightnessContrast(p=0.1),\n",
        "        A.ColorJitter(p=0.1),\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels'], min_area=1.0, min_visibility=0.0))\n",
        "\n",
        "def get_val_test_transforms():\n",
        "    # No augmentation for validation/test, only return numpy array\n",
        "    return A.Compose([\n",
        "\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n"
      ],
      "metadata": {
        "id": "9Zp7jNGA7THe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collate function\n",
        "def train_eval_collate_fn(batch):\n",
        "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
        "    pixel_mask = torch.stack([item[\"pixel_mask\"] for item in batch])\n",
        "    labels = [item[\"labels\"] for item in batch] # Keep labels as a list of dicts\n",
        "\n",
        "    # Only return items needed by the model's forward pass + labels for loss\n",
        "    # The Trainer will pass these as keyword arguments to model.forward(**batch_dict)\n",
        "    return {\n",
        "        'pixel_values': pixel_values,\n",
        "        'pixel_mask': pixel_mask,\n",
        "        'labels': labels, # Trainer needs this for loss calculation\n",
        "        # Do NOT include 'orig_size' or 'image_id_str' here\n",
        "    }\n",
        "\n",
        "# Collate function for manual evaluation loop (test set)\n",
        "# This provides all information needed for inference, post-processing, and COCO formatting.\n",
        "def test_collate_fn(batch):\n",
        "     pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
        "     pixel_mask = torch.stack([item[\"pixel_mask\"] for item in batch])\n",
        "     labels = [item[\"labels\"] for item in batch] # Keep labels as a list of dicts\n",
        "     orig_sizes = torch.stack([item[\"orig_size\"] for item in batch])\n",
        "     image_id_strs = [item[\"image_id_str\"] for item in batch]\n",
        "\n",
        "     # Return all needed info, including extra keys for evaluation logic\n",
        "     return {\n",
        "         'pixel_values': pixel_values,\n",
        "         'pixel_mask': pixel_mask,\n",
        "         'labels': labels, # Still include labels, useful potentially, but orig_sizes/image_id_strs are the critical extras\n",
        "         'orig_sizes': orig_sizes, # Needed for post-processing\n",
        "         'image_id_strs': image_id_strs # Needed for COCO formatting\n",
        "     }\n",
        "\n",
        "# Helper to create COCO-style annotation file for evaluation\n",
        "def convert_to_coco_format(dataset, output_file):\n",
        "    \"\"\"Converts dataset annotations to COCO format JSON.\"\"\"\n",
        "    coco_output = {\n",
        "        \"images\": [],\n",
        "        \"annotations\": [],\n",
        "        \"categories\": [{\"id\": v, \"name\": k, \"supercategory\": \"cell\"} for k, v in dataset.name_to_id.items()]\n",
        "    }\n",
        "    ann_id_counter = 1\n",
        "    for idx in tqdm(range(len(dataset)), desc=f\"Converting {Path(output_file).name} to COCO format\"):\n",
        "        image_id_str = dataset.image_ids_str[idx] # Use the original image_id_str\n",
        "        img_data = dataset.annotations[image_id_str] # Get pre-loaded data\n",
        "        img_h, img_w = img_data['size'].tolist() # Get size from tensor\n",
        "\n",
        "        coco_output[\"images\"].append({\n",
        "            \"id\": image_id_str, # Use index as image ID for consistency with __getitem__\n",
        "            \"file_name\": f\"{image_id_str}.jpg\",\n",
        "            \"height\": img_h,\n",
        "            \"width\": img_w,\n",
        "        })\n",
        "\n",
        "        boxes_pascal = img_data['boxes'] # Format [xmin, ymin, xmax, ymax]\n",
        "        labels = img_data['labels']\n",
        "\n",
        "        for box, label in zip(boxes_pascal, labels):\n",
        "            xmin, ymin, xmax, ymax = box.tolist()\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "            if width <= 0 or height <= 0: # Skip invalid boxes\n",
        "                continue\n",
        "            coco_bbox = [xmin, ymin, width, height] # COCO format [xmin, ymin, width, height]\n",
        "            area = width * height\n",
        "\n",
        "            coco_output[\"annotations\"].append({\n",
        "                \"id\": ann_id_counter,\n",
        "                \"image_id\": image_id_str, # Match image id\n",
        "                \"category_id\": label.item(),\n",
        "                \"bbox\": coco_bbox,\n",
        "                \"area\": area,\n",
        "                \"iscrowd\": 0,\n",
        "                # \"segmentation\": [], # Object detection doesn't need segmentation\n",
        "            })\n",
        "            ann_id_counter += 1\n",
        "\n",
        "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(coco_output, f, indent=4)\n",
        "    print(f\"Saved COCO format annotations to {output_file}\")"
      ],
      "metadata": {
        "id": "Q6qEXzQn7V-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches # Import the patches module\n",
        "\n",
        "# Helper to visualize annotations/predictions\n",
        "def plot_detections(image_path, annotations=None, predictions=None, id_to_label=None, score_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Plots an image with bounding boxes.\n",
        "    Args:\n",
        "        image_path (Path or str): Path to the image file.\n",
        "        annotations (dict or None): Ground truth annotations {'boxes': [[x1,y1,x2,y2],...], 'labels': [id,...]} (Pascal VOC format).\n",
        "        predictions (dict or None): Model predictions {'boxes': [[x1,y1,x2,y2],...], 'scores': [s,...], 'labels': [id,...]} (Pascal VOC format).\n",
        "        id_to_label (dict): Mapping from label ID to name.\n",
        "        score_threshold (float): Minimum score to display prediction boxes.\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    ax = plt.gca()\n",
        "    ax.imshow(image)\n",
        "\n",
        "    if id_to_label is None:\n",
        "        id_to_label = {0: 'background', 1: 'RBC', 2: 'WBC', 3: 'Platelets'} # Default for BCCD\n",
        "\n",
        "    # Plot ground truth boxes\n",
        "    if annotations and len(annotations.get('boxes', [])) > 0:\n",
        "        for box, label_id in zip(annotations['boxes'], annotations['labels']):\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                     linewidth=1, edgecolor='g', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            label_name = id_to_label.get(int(label_id) if isinstance(label_id, torch.Tensor) else label_id, f'ID {label_id}')\n",
        "            ax.text(xmin, ymin - 5, f'{label_name} GT', color='g', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, pad=0))\n",
        "\n",
        "    # Plot predicted boxes\n",
        "    if predictions and len(predictions.get('boxes', [])) > 0:\n",
        "         scores = predictions.get('scores', [])\n",
        "         labels = predictions.get('labels', [])\n",
        "         boxes = predictions.get('boxes', [])\n",
        "         # Filter by score threshold\n",
        "         filtered_preds = [(s, l, b) for s, l, b in zip(scores, labels, boxes) if s >= score_threshold]\n",
        "\n",
        "         if filtered_preds:\n",
        "             for score, label_id, box in filtered_preds:\n",
        "                 xmin, ymin, xmax, ymax = box\n",
        "                 rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                          linewidth=1, edgecolor='r', facecolor='none', linestyle='dashed')\n",
        "                 ax.add_patch(rect)\n",
        "                 label_name = id_to_label.get(label_id, f'ID {label_id}')\n",
        "                 ax.text(xmin, ymax + 5, f'{label_name}: {score:.2f}', color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, pad=0))\n",
        "\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Detections for {Path(image_path).name}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "r0f-YGcxf76z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define paths and Parameters ---\n",
        "DATA_DIR = Path(\"/content/BCCD_Dataset/BCCD\")\n",
        "IMAGE_DIR = DATA_DIR / \"JPEGImages\"\n",
        "ANNOT_DIR = DATA_DIR / \"Annotations\"\n",
        "IMAGE_SETS_DIR = DATA_DIR / \"ImageSets\" / \"Main\"\n",
        "\n",
        "TRAIN_SET_PATH = IMAGE_SETS_DIR / \"train.txt\"\n",
        "VAL_SET_PATH = IMAGE_SETS_DIR / \"val.txt\"\n",
        "TEST_SET_PATH = IMAGE_SETS_DIR / \"test.txt\"\n",
        "\n",
        "# Use standard DETR processor\n",
        "PRETRAINED_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
        "image_processor = DetrImageProcessor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
        "\n",
        "# Define Label Mapping (must be consistent)\n",
        "CLASS_NAMES = ['RBC', 'WBC', 'Platelets']\n",
        "LABEL_MAP_CATEGORY_ID = {'RBC': 1, 'WBC': 2, 'Platelets': 3}\n",
        "ID_TO_LABEL_WITH_BACKGROUND = {0: 'background', 1: 'RBC', 2: 'WBC', 3: 'Platelets'}\n",
        "LABEL_TO_ID_WITH_BACKGROUND = {v:k for k,v in ID_TO_LABEL_WITH_BACKGROUND.items()}\n",
        "\n",
        "# Create Datasets\n",
        "print(\"Creating Training Dataset...\")\n",
        "train_dataset = BCCDDataset(\n",
        "    image_dir=IMAGE_DIR,\n",
        "    annot_dir=ANNOT_DIR,\n",
        "    image_set_path=TRAIN_SET_PATH,\n",
        "    processor=image_processor,\n",
        "    transforms=get_train_transforms(), # Enable if needed, but processor handles resize/norm\n",
        "    name_to_id=LABEL_MAP_CATEGORY_ID\n",
        ")\n",
        "\n",
        "print(\"\\nCreating Validation Dataset...\")\n",
        "val_dataset = BCCDDataset(\n",
        "    image_dir=IMAGE_DIR,\n",
        "    annot_dir=ANNOT_DIR,\n",
        "    image_set_path=VAL_SET_PATH,\n",
        "    processor=image_processor,\n",
        "    transforms=get_val_test_transforms(), # No augmentation\n",
        "    name_to_id=LABEL_MAP_CATEGORY_ID\n",
        ")\n",
        "\n",
        "print(\"\\nCreating Test Dataset...\")\n",
        "test_dataset = BCCDDataset(\n",
        "    image_dir=IMAGE_DIR,\n",
        "    annot_dir=ANNOT_DIR,\n",
        "    image_set_path=TEST_SET_PATH,\n",
        "    processor=image_processor,\n",
        "    transforms=get_val_test_transforms(), # No augmentation\n",
        "    name_to_id=LABEL_MAP_CATEGORY_ID\n",
        ")\n",
        "\n",
        "# Check dataset sizes\n",
        "print(f\"\\nDataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "i-_rOoHa67sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify label mapping consistency\n",
        "assert train_dataset.name_to_id == LABEL_MAP_CATEGORY_ID\n",
        "assert val_dataset.name_to_id == LABEL_MAP_CATEGORY_ID\n",
        "assert test_dataset.name_to_id == LABEL_MAP_CATEGORY_ID\n",
        "assert train_dataset.id_to_label == ID_TO_LABEL_WITH_BACKGROUND # Check full map including background\n",
        "print(\"Label map verified.\")\n",
        "\n",
        "# Visualize a sample from the training dataset (before processor transformation)\n",
        "print(\"\\nVisualizing a sample from the training dataset (original image + GT boxes):\")\n",
        "sample_idx = 0\n",
        "sample_item = train_dataset[sample_idx]\n",
        "sample_image_id_str = sample_item['image_id_str']\n",
        "sample_original_target = train_dataset.annotations[sample_image_id_str] # Get original annotations\n",
        "\n",
        "# Load original image again for visualization\n",
        "sample_image_path = IMAGE_DIR / f\"{sample_image_id_str}.jpg\"\n",
        "\n",
        "plot_detections(\n",
        "    sample_image_path,\n",
        "    annotations={'boxes': sample_original_target['boxes'], 'labels': sample_original_target['labels']},\n",
        "    id_to_label=train_dataset.id_to_label # Use the full ID to label map\n",
        ")"
      ],
      "metadata": {
        "id": "g4ThnEEg7guV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert validation and test sets to COCO format for evaluation\n",
        "VAL_COCO_ANNOT_FILE = \"./val_coco_annotations.json\"\n",
        "TEST_COCO_ANNOT_FILE = \"./test_coco_annotations.json\"\n",
        "convert_to_coco_format(val_dataset, VAL_COCO_ANNOT_FILE)\n",
        "convert_to_coco_format(test_dataset, TEST_COCO_ANNOT_FILE)"
      ],
      "metadata": {
        "id": "PxbCnJxpg7YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# Cell 3: moe_layer.py (Revised for simplicity and direct use of DetrMLP)\n",
        "# -------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers.models.detr.modeling_detr import DetrMLPPredictionHead, DetrConfig # Need config for DetrMLP defaults if not passed\n",
        "\n",
        "\n",
        "class MoEFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    Mixture of Experts FFN Layer: replaces (fc1 -> activation -> fc2) block.\n",
        "    Each expert is (fc1 -> activation -> fc2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, num_experts, activation_fn=nn.ReLU()):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_experts = num_experts\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "        # Each expert is a sequential: fc1 -> activation -> fc2\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                self._clone_activation(), # ensure separate instances\n",
        "                nn.Linear(hidden_dim, input_dim)\n",
        "            ) for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "    def _clone_activation(self):\n",
        "        \"\"\"Clone activation function to avoid shared states.\"\"\"\n",
        "        return type(self.activation_fn)()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states: (batch_size, seq_len, input_dim)\n",
        "        Returns:\n",
        "            (batch_size, seq_len, input_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, dim = hidden_states.shape\n",
        "        hidden_states_flat = hidden_states.view(-1, dim) # (batch*seq_len, dim)\n",
        "\n",
        "        # Gating\n",
        "        gate_logits = self.gate(hidden_states_flat) # (batch*seq_len, num_experts)\n",
        "        gate_weights = F.softmax(gate_logits, dim=-1)\n",
        "\n",
        "        # Expert outputs\n",
        "        expert_outputs = []\n",
        "        for expert in self.experts:\n",
        "            expert_outputs.append(expert(hidden_states_flat)) # (batch*seq_len, dim)\n",
        "\n",
        "        expert_outputs_stacked = torch.stack(expert_outputs, dim=1) # (batch*seq_len, num_experts, dim)\n",
        "\n",
        "        final_output_flat = torch.sum(gate_weights.unsqueeze(-1) * expert_outputs_stacked, dim=1) # (batch*seq_len, dim)\n",
        "\n",
        "        final_output = final_output_flat.view(batch_size, seq_len, dim)\n",
        "\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "jxkSPhrK7uEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# Cell 4: detr_moe.py\n",
        "# -------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DetrForObjectDetection, DetrConfig\n",
        "# from moe_layer import MoEFFN # Assuming MoEFFN is defined in moe_layer.py or previous cell\n",
        "\n",
        "def replace_ffn_with_moe(model: DetrForObjectDetection, num_experts: int):\n",
        "    \"\"\"\n",
        "    Replaces (fc1 -> activation -> fc2) blocks in encoder and decoder layers with MoEFFN.\n",
        "    \"\"\"\n",
        "    config = model.config\n",
        "    d_model = config.d_model\n",
        "    encoder_hidden_dim = getattr(config, 'encoder_ffn_dim', d_model * 4)\n",
        "    decoder_hidden_dim = getattr(config, 'decoder_ffn_dim', d_model * 4)\n",
        "\n",
        "    print(f\"Replacing FFN with MoE (num_experts={num_experts})...\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Encoder\n",
        "    if hasattr(model.model.encoder, 'layers'):\n",
        "        print(f\"Found {len(model.model.encoder.layers)} encoder layers.\")\n",
        "        for i, layer in enumerate(model.model.encoder.layers):\n",
        "            if hasattr(layer, 'fc1') and hasattr(layer, 'fc2'):\n",
        "                print(f\"Replacing Encoder Layer {i} FFN with MoEFFN.\")\n",
        "\n",
        "                # Ensure the original activation function is correctly captured if it's a module\n",
        "                original_activation = layer.activation_fn\n",
        "                if not isinstance(original_activation, nn.Module):\n",
        "                     # Handle cases where activation_fn might be a functional (like F.relu)\n",
        "                     # Defaulting to nn.ReLU() here, adjust if needed based on actual config\n",
        "                     print(f\"Warning: Encoder Layer {i} activation_fn is not an nn.Module ({type(original_activation)}). Using nn.ReLU for MoE experts.\")\n",
        "                     original_activation = nn.ReLU()\n",
        "\n",
        "                moe_ffn = MoEFFN(\n",
        "                    input_dim=d_model,\n",
        "                    hidden_dim=encoder_hidden_dim,\n",
        "                    num_experts=num_experts,\n",
        "                    activation_fn=original_activation\n",
        "                ).to(device)\n",
        "\n",
        "                layer.fc1 = moe_ffn\n",
        "                # Replace activation and fc2 with Identity to pass the MoE output through\n",
        "                # Make sure Identity layers are also on the correct device\n",
        "                layer.activation_fn = nn.Identity().to(device)\n",
        "                layer.fc2 = nn.Identity().to(device)\n",
        "\n",
        "                print(f\" - Replaced fc1 with MoEFFN({num_experts} experts) on device {layer.fc1.gate.weight.device}\")\n",
        "                print(f\" - Set activation_fn to Identity\")\n",
        "                print(f\" - Set fc2 to Identity\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Warning: Encoder Layer {i} missing standard FFN attributes (fc1, fc2, activation_fn). Skipping replacement.\")\n",
        "    else:\n",
        "        print(\"Warning: No encoder layers found or attribute 'layers' missing.\")\n",
        "\n",
        "    # Decoder\n",
        "    if hasattr(model.model.decoder, 'layers'):\n",
        "        print(f\"Found {len(model.model.decoder.layers)} decoder layers.\")\n",
        "        for i, layer in enumerate(model.model.decoder.layers):\n",
        "             # Check for the main FFN components in decoder layers\n",
        "             if hasattr(layer, 'fc1') and hasattr(layer, 'fc2') and hasattr(layer, 'activation_fn'):\n",
        "                print(f\"Replacing Decoder Layer {i} FFN components with MoEFFN.\")\n",
        "                original_activation = layer.activation_fn\n",
        "                if not isinstance(original_activation, nn.Module):\n",
        "                     print(f\"Warning: Decoder Layer {i} activation_fn is not an nn.Module ({type(original_activation)}). Using nn.ReLU for MoE experts.\")\n",
        "                     original_activation = nn.ReLU()\n",
        "\n",
        "                moe_ffn = MoEFFN(\n",
        "                    input_dim=d_model,\n",
        "                    hidden_dim=decoder_hidden_dim,\n",
        "                    num_experts=num_experts,\n",
        "                    activation_fn=original_activation\n",
        "                ).to(device) # Move the new module to the correct device\n",
        "\n",
        "                layer.fc1 = moe_ffn\n",
        "                layer.activation_fn = nn.Identity().to(device)\n",
        "                layer.fc2 = nn.Identity().to(device)\n",
        "\n",
        "                print(f\" - Replaced fc1 with MoEFFN({num_experts} experts) on device {layer.fc1.gate.weight.device}\")\n",
        "                print(f\" - Set activation_fn to Identity\")\n",
        "                print(f\" - Set fc2 to Identity\")\n",
        "             else:\n",
        "                 print(f\"Warning: Decoder Layer {i} missing standard FFN attributes (fc1, fc2, activation_fn). Skipping replacement.\")\n",
        "    else:\n",
        "        print(\"Warning: No decoder layers found or attribute 'layers' missing.\")\n",
        "\n",
        "    print(\"FFN component replacement complete.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "MZIPr4uH79-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# Cell 5: engine.py (Evaluation Logic)\n",
        "# -------------------------------------\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def prepare_for_coco_detection(predictions, processor, id_to_label):\n",
        "    \"\"\"Converts model outputs to COCO format for evaluation.\"\"\"\n",
        "    coco_results = []\n",
        "    background_id = 0\n",
        "    for image_id_str, prediction in predictions.items():\n",
        "        if prediction is None or 'scores' not in prediction or len(prediction['scores']) == 0:\n",
        "            continue\n",
        "\n",
        "        scores = prediction['scores'].tolist()\n",
        "        labels = prediction['labels'].tolist()\n",
        "        boxes = prediction['boxes'].tolist() # Expected format [xmin, ymin, xmax, ymax]\n",
        "\n",
        "        for score, label_id, box in zip(scores, labels, boxes):\n",
        "            if label_id == background_id:\n",
        "                continue\n",
        "            # Ensure box is in [xmin, ymin, width, height] format for COCO\n",
        "            xmin, ymin, xmax, ymax = box\n",
        "            width = xmax - xmin\n",
        "            height = ymax - ymin\n",
        "            if width <= 0 or height <= 0:\n",
        "                continue\n",
        "            coco_box = [xmin, ymin, width, height]\n",
        "\n",
        "            result = {\n",
        "                'image_id': image_id_str, # Must match the image_id in the ground truth COCO file\n",
        "                'category_id': label_id,\n",
        "                'bbox': coco_box,\n",
        "                'score': score,\n",
        "            }\n",
        "            coco_results.append(result)\n",
        "    return coco_results\n",
        "\n",
        "def compute_coco_metrics(model, processor, dataloader, coco_gt_file, device, id_to_label):\n",
        "    \"\"\"Runs inference and computes COCO metrics.\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    all_predictions = {}\n",
        "\n",
        "    print(f\"Loading ground truth from {coco_gt_file}\")\n",
        "    try:\n",
        "        coco_gt = COCO(coco_gt_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ground truth file {coco_gt_file}: {e}\")\n",
        "        print(\"Cannot compute COCO metrics.\")\n",
        "        return {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0, \"error\": f\"Failed to load GT: {e}\"}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            # Retrieve original image IDs and sizes from the batch\n",
        "            image_id_strs = batch['image_id_strs'] # <-- Get original image_id_strs\n",
        "            orig_sizes = batch['orig_sizes'].to(device) # Original image sizes [H, W]\n",
        "\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            pixel_mask = batch[\"pixel_mask\"].to(device)\n",
        "\n",
        "            # Ensure the model is explicitly in eval mode\n",
        "            model.eval()\n",
        "            outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "            # Post-process results\n",
        "            results = processor.post_process_object_detection(\n",
        "                outputs,\n",
        "                threshold=0.01, # Use a threshold appropriate for evaluation\n",
        "                target_sizes=orig_sizes\n",
        "            ) # List of dicts per image: {'scores', 'labels', 'boxes'}\n",
        "\n",
        "            # Store predictions mapped by their original image ID\n",
        "            for img_id_str, res in zip(image_id_strs, results):\n",
        "                all_predictions[img_id_str] = res\n",
        "\n",
        "\n",
        "    # Convert predictions to COCO format\n",
        "    coco_formatted_preds = prepare_for_coco_detection(all_predictions, processor, id_to_label)\n",
        "\n",
        "    if not coco_formatted_preds:\n",
        "        print(\"No predictions generated above threshold=0.01 after post-processing. Returning zero metrics.\")\n",
        "        return {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0}\n",
        "\n",
        "    # Save predictions to a temp file\n",
        "    results_file = \"coco_predictions.json\"\n",
        "    try:\n",
        "        with open(results_file, \"w\") as f:\n",
        "            json.dump(coco_formatted_preds, f, indent=4)\n",
        "        print(f\"Saved {len(coco_formatted_preds)} predictions to {results_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving predictions to {results_file}: {e}\")\n",
        "        return {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0, \"error\": f\"Failed to save preds: {e}\"}\n",
        "\n",
        "    # Load predictions using COCO API\n",
        "    try:\n",
        "        coco_dt = coco_gt.loadRes(results_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading prediction results into pycocotools from {results_file}: {e}\")\n",
        "        print(\"This might happen if the JSON format is incorrect or empty.\")\n",
        "        # os.remove(results_file) # Clean up temp file even on error\n",
        "        return {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0, \"error\": f\"Failed loadRes: {e}\"}\n",
        "\n",
        "    # Run COCO evaluation\n",
        "    print(\"Running COCO evaluation...\")\n",
        "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
        "\n",
        "    try:\n",
        "        coco_eval.evaluate()\n",
        "        coco_eval.accumulate()\n",
        "        print(\"\\nCOCO Evaluation Summary:\")\n",
        "        coco_eval.summarize()\n",
        "\n",
        "        # Extract key metrics (mAP at IoU=0.50:0.95, mAP at IoU=0.50, mAP at IoU=0.75)\n",
        "        # Index 0: Mean AP @ IoU=[0.50:0.95], area=all, maxDets=100\n",
        "        # Index 1: Mean AP @ IoU=0.50, area=all, maxDets=100\n",
        "        # Index 2: Mean AP @ IoU=0.75, area=all, maxDets=100\n",
        "        map_iou_50_95 = coco_eval.stats[0]\n",
        "        map_iou_50 = coco_eval.stats[1]\n",
        "        map_iou_75 = coco_eval.stats[2]\n",
        "\n",
        "        metrics = {\n",
        "            \"mAP\": round(map_iou_50_95, 4),\n",
        "            \"mAP_50\": round(map_iou_50, 4),\n",
        "            \"mAP_75\": round(map_iou_75, 4),\n",
        "        }\n",
        "        print(f\"Computed Metrics: {metrics}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during COCO evaluation: {e}\")\n",
        "        metrics = {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0, \"error\": f\"Eval failed: {e}\"}\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "cTX0gwdt7_5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# Cell 6: Main Training and Comparison\n",
        "# -------------------------------------\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate # Hugging Face Evaluate library\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_DIR_BASE = \"./detr_bccd_results\"\n",
        "NUM_TRAIN_EPOCHS = 2\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 4 # Adjust based on GPU memory\n",
        "PER_DEVICE_EVAL_BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-4 # Standard DETR LR\n",
        "WEIGHT_DECAY = 1e-4\n",
        "LOGGING_STEPS = 50\n",
        "EVAL_STEPS = 200 # Evaluate less frequently maybe\n",
        "SAVE_STEPS = 200 # Save checkpoints less frequently\n",
        "GRAD_ACCUM_STEPS = 2 # Use gradient accumulation if batch size is small\n",
        "\n",
        "NUM_EXPERTS_MOE = 4 # Number of experts for the MoE model\n",
        "\n",
        "# --- Helper Function for Trainer ---\n",
        "# Define compute_metrics function for Hugging Face Trainer\n",
        "# The default Trainer expects metrics during training based on eval_dataset.\n",
        "# It passes 'eval_pred' which contains predictions and labels.\n",
        "# However, COCO evaluation needs the whole dataset processed.\n",
        "# We will run our custom COCO evaluation *after* training.\n",
        "# The simple_compute_metrics is mainly for Trainer's internal logging (e.g., eval_loss).\n",
        "def simple_compute_metrics(eval_pred):\n",
        "    # This function runs on a *batch* during Trainer's eval_strategy.\n",
        "    # It's not suitable for full COCO mAP calculation.\n",
        "    # We can compute batch loss here if needed, but the Trainer already logs it.\n",
        "    # Return an empty dict or a simple placeholder.\n",
        "    return {} # No metrics computed here\n",
        "\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model(\n",
        "    model_name_or_path, model_output_dir, model_init_config,\n",
        "    image_processor, train_dataset, val_dataset, test_dataset, is_moe=False, num_experts=0\n",
        "    ):\n",
        "    print(f\"\\n--- Training Model: {'DETR+MoE' if is_moe else 'Standard DETR'} ---\")\n",
        "    print(f\"Initialization: {model_init_config['type']}\")\n",
        "    print(f\"Output Dir: {model_output_dir}\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1. Load Model Configuration\n",
        "    config = DetrConfig.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        # num_labels should match the number of classes *including* background for the model's classification head\n",
        "        num_labels=len(ID_TO_LABEL_WITH_BACKGROUND),\n",
        "        id2label=ID_TO_LABEL_WITH_BACKGROUND,\n",
        "        label2id=LABEL_TO_ID_WITH_BACKGROUND,\n",
        "        ignore_mismatched_sizes=True # Important if changing num_labels\n",
        "    )\n",
        "    print(f\"Model config num_labels: {config.num_labels}\")\n",
        "\n",
        "    # 2. Instantiate Model based on config and initialization strategy\n",
        "    model = None\n",
        "    if model_init_config[\"type\"] == \"full_pretrained\":\n",
        "        print(f\"Loading fully pre-trained weights from {model_name_or_path}\")\n",
        "        model = DetrForObjectDetection.from_pretrained(\n",
        "            model_name_or_path,\n",
        "            config=config,\n",
        "            ignore_mismatched_sizes=True # Allow mismatch in classification head\n",
        "        )\n",
        "\n",
        "        model.to(device)\n",
        "        print(f\"Model moved to {device}\")\n",
        "\n",
        "    elif model_init_config[\"type\"] == \"pretrained_backbone_random_transformer\":\n",
        "        print(f\"Loading pre-trained backbone, random transformer from {model_name_or_path}\")\n",
        "        # Load config first, then instantiate with random weights based on config\n",
        "        model = DetrForObjectDetection(config) # This initializes all weights randomly\n",
        "\n",
        "        model.to(device)\n",
        "        print(f\"Empty model moved to {device} before loading backbone.\")\n",
        "\n",
        "        # Load only the backbone weights from the pre-trained model\n",
        "        pretrained_model = DetrForObjectDetection.from_pretrained(model_name_or_path, map_location='cpu')\n",
        "        print(\"Loading pre-trained backbone state dict...\")\n",
        "        model.model.backbone.load_state_dict(pretrained_model.model.backbone.state_dict())\n",
        "        print(\"Backbone weights loaded.\")\n",
        "        del pretrained_model # Free up memory\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Transformer encoder/decoder/head weights remain randomly initialized by default\n",
        "        print(\"Transformer encoder, decoder, and head weights remain randomly initialized.\")\n",
        "\n",
        "    elif model_init_config[\"type\"] == \"random_init\":\n",
        "         print(\"Loading model with random weights (using config)\")\n",
        "         # Note: This initializes backbone randomly too, unless config specifies otherwise\n",
        "         model = DetrForObjectDetection(config)\n",
        "         model.to(device)\n",
        "         print(f\"Model loaded and moved to {device}\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model_init_config type: {model_init_config['type']}\")\n",
        "\n",
        "    # model.to(device)\n",
        "    # print(f\"Model moved to {device}\")\n",
        "\n",
        "    # 3. Apply MoE if specified\n",
        "    if is_moe:\n",
        "        if next(model.parameters()).device.type == 'meta':\n",
        "            raise RuntimeError(\"Model parameters are still on meta device before MoE replacement!\")\n",
        "        model = replace_ffn_with_moe(model, num_experts)\n",
        "        print(f\"MoE layers integrated with {num_experts} experts.\")\n",
        "\n",
        "\n",
        "\n",
        "    # 4. Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=model_output_dir,\n",
        "        num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        lr_scheduler_type=\"cosine\", # Or \"linear\"\n",
        "        logging_dir=f\"{model_output_dir}/logs\",\n",
        "        logging_steps=LOGGING_STEPS,\n",
        "        do_eval=True,\n",
        "        eval_strategy=\"steps\", # Evaluate periodically\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=SAVE_STEPS,\n",
        "        eval_steps=EVAL_STEPS,\n",
        "        save_total_limit=3, # Keep more checkpoints maybe\n",
        "        load_best_model_at_end=True, # Load best model based on eval metric\n",
        "        metric_for_best_model=\"eval_loss\", # Use eval loss reported by Trainer (or implement custom metric)\n",
        "        greater_is_better=False, # Lower loss is better\n",
        "        # metric_for_best_model=\"mAP\", # Requires custom integration with compute_metrics\n",
        "        report_to=\"tensorboard\", # or \"wandb\"\n",
        "        fp16=torch.cuda.is_available(), # Use mixed precision if available\n",
        "        push_to_hub=False,\n",
        "        remove_unused_columns=False, # Important for DETR labels format\n",
        "    )\n",
        "\n",
        "    # 5. Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset, # Use validation set for evaluation during training\n",
        "        tokenizer=image_processor, # Pass processor as tokenizer for consistency\n",
        "        data_collator=train_eval_collate_fn,\n",
        "        compute_metrics=simple_compute_metrics, # Use simple one for Trainer, run full eval later\n",
        "    )\n",
        "\n",
        "    # 6. Train\n",
        "    print(\"Starting training...\")\n",
        "    train_start_time = time.time()\n",
        "    train_result = trainer.train()\n",
        "    train_end_time = time.time()\n",
        "    print(f\"Training finished in {train_end_time - train_start_time:.2f} seconds.\")\n",
        "\n",
        "    # Log training metrics\n",
        "    metrics = train_result.metrics\n",
        "    trainer.log_metrics(\"train\", metrics)\n",
        "    trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "\n",
        "    # 7. Save final model and processor\n",
        "    print(\"\\nSaving final model...\")\n",
        "    final_model_dir = os.path.join(model_output_dir, \"final_model\")\n",
        "    trainer.save_model(final_model_dir)\n",
        "    if image_processor:\n",
        "        image_processor.save_pretrained(final_model_dir)\n",
        "    print(f\"Final model saved to {final_model_dir}\")\n",
        "\n",
        "    # 8. Evaluate on Test Set using COCO metrics\n",
        "    print(\"\\nEvaluating on Test Set...\")\n",
        "    # Load the best model saved by the Trainer (based on eval_loss)\n",
        "    best_model_checkpoint_path = trainer.state.best_model_checkpoint\n",
        "    loaded_model_path = best_model_checkpoint_path if best_model_checkpoint_path else final_model_dir\n",
        "\n",
        "    if loaded_model_path:\n",
        "        print(f\"Loading model for test evaluation from: {loaded_model_path}\")\n",
        "        try:\n",
        "             # Need to load the model structure correctly, especially if MoE was applied\n",
        "             # The config should be saved with the model.\n",
        "             model_to_eval = DetrForObjectDetection.from_pretrained(loaded_model_path)\n",
        "             model_to_eval.to(DEVICE)\n",
        "             # If MoE layers were saved correctly, they should be loaded.\n",
        "             # Basic check:\n",
        "             has_moe_encoder = any(isinstance(l.ffn, MoEFFN) for l in model_to_eval.model.encoder.layers if hasattr(l, 'ffn'))\n",
        "             has_moe_decoder = any(isinstance(l.ffn, MoEFFN) for l in model_to_eval.model.decoder.layers if hasattr(l, 'ffn'))\n",
        "             print(f\"Loaded model has MoE layers (Encoder): {has_moe_encoder}, (Decoder): {has_moe_decoder}\")\n",
        "             # If somehow not loaded, re-apply, but this indicates a saving/loading issue\n",
        "             # if is_moe and (not has_moe_encoder or not has_moe_decoder):\n",
        "             #      print(\"Warning: MoE layers not detected in loaded checkpoint, attempting to re-apply.\")\n",
        "             #      model_to_eval = replace_ffn_with_moe(model_to_eval, num_experts)\n",
        "\n",
        "             loaded_processor = DetrImageProcessor.from_pretrained(loaded_model_path)\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading model from {loaded_model_path}: {e}\")\n",
        "             print(\"Cannot perform test evaluation.\")\n",
        "             return {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0, \"error\": f\"Failed to load model for eval: {e}\"}\n",
        "    else:\n",
        "        print(\"No model found to load for test evaluation.\")\n",
        "        return {\"mAP\": 0.0, \"mAP_50\": 0.0, \"mAP_75\": 0.0, \"error\": \"No model saved/loaded\"}\n",
        "\n",
        "\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=PER_DEVICE_EVAL_BATCH_SIZE, collate_fn=test_collate_fn)\n",
        "\n",
        "    # --- Debugging Step: Visualize Predictions ---\n",
        "    print(\"\\nVisualizing predictions on a sample from the Test Set...\")\n",
        "    sample_eval_batch = next(iter(test_dataloader))\n",
        "    # Take the first image from the batch\n",
        "    sample_image_id_str = sample_eval_batch['image_id_strs'][0]\n",
        "    sample_pixel_values = sample_eval_batch['pixel_values'][0].unsqueeze(0).to(DEVICE) # Add batch dim\n",
        "    sample_pixel_mask = sample_eval_batch['pixel_mask'][0].unsqueeze(0).to(DEVICE)\n",
        "    sample_orig_size = sample_eval_batch['orig_sizes'][0].unsqueeze(0).to(DEVICE) # Add batch dim\n",
        "\n",
        "\n",
        "    model_to_eval.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_outputs = model_to_eval(pixel_values=sample_pixel_values, pixel_mask=sample_pixel_mask)\n",
        "        sample_results = loaded_processor.post_process_object_detection(\n",
        "            sample_outputs,\n",
        "            threshold=0.5, # Use a higher threshold for visualization clarity\n",
        "            target_sizes=sample_orig_size\n",
        "        )[0] # Get results for the single image\n",
        "\n",
        "\n",
        "    # Need the original image path for plotting\n",
        "    sample_image_path_for_plot = test_dataset.image_dir / f\"{sample_image_id_str}.jpg\"\n",
        "    # Need original GT annotations for plotting\n",
        "    sample_original_target_for_plot = test_dataset.annotations[sample_image_id_str]\n",
        "\n",
        "\n",
        "    # Convert predicted boxes back to Pascal VOC format [x1, y1, x2, y2] for plotting function\n",
        "    # The processor outputs [x1, y1, x2, y2] in absolute coordinates already! Good.\n",
        "    sample_predictions_for_plot = {\n",
        "        'boxes': sample_results['boxes'].tolist(),\n",
        "        'scores': sample_results['scores'].tolist(),\n",
        "        'labels': sample_results['labels'].tolist() # These are category_ids (1, 2, 3)\n",
        "    }\n",
        "\n",
        "    plot_detections(\n",
        "        sample_image_path_for_plot,\n",
        "        annotations={'boxes': sample_original_target_for_plot['boxes'].tolist(), 'labels': sample_original_target_for_plot['labels'].tolist()},\n",
        "        predictions=sample_predictions_for_plot,\n",
        "        id_to_label=test_dataset.id_to_label, # Use the full ID to label map\n",
        "        score_threshold=0.5 # Visualize with a higher score threshold\n",
        "    )\n",
        "    # End Debugging Step\n",
        "\n",
        "    test_metrics = compute_coco_metrics(\n",
        "        model=model_to_eval,\n",
        "        processor=loaded_processor,\n",
        "        dataloader=test_dataloader,\n",
        "        coco_gt_file=TEST_COCO_ANNOT_FILE,\n",
        "        device=DEVICE,\n",
        "        id_to_label=test_dataset.id_to_label\n",
        "    )\n",
        "    print(f\"Test Set Metrics: {test_metrics}\")\n",
        "\n",
        "    # Clean up memory\n",
        "    del model_to_eval, trainer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "results = {}"
      ],
      "metadata": {
        "id": "UCx42x6c8CvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Select which experiments to run ---\n",
        "# Choose one or two to start, as training DETR takes time.\n",
        "# Let's start by debugging the standard DETR to ensure the pipeline works.\n",
        "\n",
        "print(\"\\n\\n === Running Experiment 1: Standard DETR (Fully Pre-trained) ===\")\n",
        "init_config_1 = {\"type\": \"full_pretrained\"}\n",
        "output_dir_1 = os.path.join(OUTPUT_DIR_BASE, \"detr_standard_full_pretrained\")\n",
        "results[\"detr_standard_full_pretrained\"] = train_model(\n",
        "    model_name_or_path=PRETRAINED_MODEL_NAME,\n",
        "    model_output_dir=output_dir_1,\n",
        "    model_init_config=init_config_1,\n",
        "    image_processor=image_processor,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    is_moe=False\n",
        ")"
      ],
      "metadata": {
        "id": "6Op2Hk6oV2XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Print Final Results Summary ---\n",
        "print(\"\\n\\n--- Final Comparison Results ---\")\n",
        "for config_name, metrics in results.items():\n",
        "    print(f\"\\nConfiguration: {config_name}\")\n",
        "    if metrics and \"error\" not in metrics:\n",
        "        print(f\"  mAP (IoU .50-.95): {metrics.get('mAP', 'N/A')}\")\n",
        "        print(f\"  mAP (IoU .50):     {metrics.get('mAP_50', 'N/A')}\")\n",
        "        print(f\"  mAP (IoU .75):     {metrics.get('mAP_75', 'N/A')}\")\n",
        "        if \"notes\" in metrics:\n",
        "             print(f\"  Notes: {metrics['notes']}\")\n",
        "    elif \"error\" in metrics:\n",
        "         print(f\"  Evaluation Error: {metrics['error']}\")\n",
        "    else:\n",
        "        print(\"  Metrics not available (training/evaluation might have failed).\")"
      ],
      "metadata": {
        "id": "dfNjQpVRKPI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\\n === Running Experiment 2: DETR + MoE (Fully Pre-trained Base) ===\")\n",
        "init_config_2 = {\"type\": \"full_pretrained\"}\n",
        "output_dir_2 = os.path.join(OUTPUT_DIR_BASE, f\"detr_moe_{NUM_EXPERTS_MOE}exp_full_pretrained\")\n",
        "results[f\"detr_moe_{NUM_EXPERTS_MOE}exp_full_pretrained\"] = train_model(\n",
        "    model_name_or_path=PRETRAINED_MODEL_NAME,\n",
        "    model_output_dir=output_dir_2,\n",
        "    model_init_config=init_config_2,\n",
        "    image_processor=image_processor,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    test_dataset=test_dataset,\n",
        "    is_moe=True,\n",
        "    num_experts=NUM_EXPERTS_MOE\n",
        ")"
      ],
      "metadata": {
        "id": "P823D3DSh4il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Print Final Results Summary ---\n",
        "print(\"\\n\\n--- Final Comparison Results ---\")\n",
        "for config_name, metrics in results.items():\n",
        "    print(f\"\\nConfiguration: {config_name}\")\n",
        "    if metrics and \"error\" not in metrics:\n",
        "        print(f\"  mAP (IoU .50-.95): {metrics.get('mAP', 'N/A')}\")\n",
        "        print(f\"  mAP (IoU .50):     {metrics.get('mAP_50', 'N/A')}\")\n",
        "        print(f\"  mAP (IoU .75):     {metrics.get('mAP_75', 'N/A')}\")\n",
        "        if \"notes\" in metrics:\n",
        "             print(f\"  Notes: {metrics['notes']}\")\n",
        "    elif \"error\" in metrics:\n",
        "         print(f\"  Evaluation Error: {metrics['error']}\")\n",
        "    else:\n",
        "        print(\"  Metrics not available (training/evaluation might have failed).\")"
      ],
      "metadata": {
        "id": "2jRLdXCLhdsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}